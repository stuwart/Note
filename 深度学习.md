### 1. 线性回归 

+ 回归函数： $y=W^TX$

+ 均方误差（损失函数）：最常用的性能度量，记为 $f(x)$ 与 $y$ 的差异

  ​				$ J(a,b) = \sum\limits_{i=1}^n(f(x^{(i)})-y^{(i)})^2 = \sum\limits_{i=1}^n(ax^{(i)}+b-y^{(i)})^2$

+ 关键：确定a和b（通过确定 $J$函数 的最小值来确定 a 和 b

  + 最小二乘法（比较少用，一般不使用）

  + 梯度下降法（通用，也可用于逻辑回归）

    ​		损失函数： $J(\theta) = \sum\limits_{i=1}^n(f(x^{(i)})-y^{(i)})^2$

    ​		公式： repeat{ $\theta_{j+1} :=\theta_j-\alpha\frac{\partial J(\theta)}{\partial \theta_j}$}

    ​		Adagrad:   $w^{t+1}=w^t-\frac{\eta^t}{\sigma^t}g^t = w^t-\frac{\eta}{\sqrt{\sum_{i=0}^t(g^i)^2}}g^t$   用以确定learning rate

  + 正规方程（小数据下速度更快）			
  
  ​		解析解：![image-20220130094039065](C:\Users\hy020\AppData\Roaming\Typora\typora-user-images\image-20220130094039065.png)

### 2.逻辑回归（简单回归）

+ 本质：假设数据服从这个分布，然后使用极大似然估计做参数的估计

+ 使用Sigmod函数作为回归函数 ：		$g(z)=\frac{1}{1+e^{-z}}$  ，$z=W^TX$ （预测结果为0~1的小数）

+ 逻辑回归模型：

  ​			$y=\frac{1}{1+e^{-(w^Tx+b)}}$

​					$ w^Tx+b=ln\frac{P(Y=1\mid x)}{1-P(Y=1\mid x)}$ 

​					$P(Y=0\mid x)= 1-\frac{1}{1+e^{-(w^Tx+b)}}$

​					$P(Y=1\mid x) = \frac{1}{1+e^{-(w^Tx+b)}}$

​					$P(正确)=(g(z))^{y^i}*(1-g(z)^{1-y^i})$



+ 选定阈值，进行分类
  + 根据需要选择阈值，一般0.5进行调动



+ 求解

  + 最大似然估计： $L(\theta)=\prod\limits_{i=1}^m(h_\theta(x^{(i)}))^{1-y^{(i)}}$    ， $h_\theta(x)=\frac{1}{1+e^{-z}}$ 

    ​				$l(\theta)=log\,L(\theta)=\sum\limits_{i=1}^m(y^{(i)}log\,h_\theta(x^{(i)})+(1-y^{(i)})log(1-h_\theta(x^{(i)})))$

    + 所得到的函数越大，说明得到的W越好

  + 损失函数（交叉熵损失函数.）：$J_{log}(\theta)=-log\,L(\theta)=-\sum\limits_{i=1}^m(y^{(i)}log\,h_\theta(x^{(i)})+(1-y^{(i)})log(1-h_\theta(x^{(i)})))$

    + 求解交叉熵损失函数：

    <img src="C:\Users\hy020\AppData\Roaming\Typora\typora-user-images\image-20220131014848398.png" style="zoom:50%;" />

### 3.分类

+ 生成模型： $P(x)=P(x|)$































### 4.SVM



### 5.卷积神经网络



### 6.循环神经网络



### 7.半监督学习



### 8.无监督学习



### 9.深度生成模型



### 10.迁移学习



### 11.RCNN



### 12.YOLO



### 13.LSTM



### 14.Transformer



### 15.XGBoost
