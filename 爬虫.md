### 1.爬虫基础

#### 				1.1 request模块

##### 									1.1.1 静态网页（直接获取）

``` python
import requests
response = requests.get(url=' ')
print(response.text)
```

对网页发起请求，服务器返回一个响应对象。包括：

+ 响应头：包含响应的状态码、日期、内容类型和编码格式等信息
+ 响应体：包含字符串形式的网页源代码



##### 				1.1.2 动态网页

  服务器返回网页模板，数据通过其他方式到达指定位置，一般包含在json中。

  ```python
  import requests
  headers = ···
  url = ···
  params = ···   #在发送请求时携带动态参数
  timeout = ···
  
  response = requests.get(url=url, headers=headers, params=params)
  print(response.json())
  ```

  ​	

  ##### 				1.1.3 获取图片

+ get()获取响应对象
+ 利用content属性获取图片（不可用text）



#### 		1.2  BeautifulSoup 模块

将对象实例化后可利用标签定位。

##### 				1.2.1 利用标签名进行定位

```python
from bs4 import BeautifulSoup
fp = open('text1.html', encoding='utf-8')  #打开
soup = BeautifulSoup(fp,'lxml')			#lxml
```





#### 	1.3 XPath表达式

+ 实例化: ` html = etree.HTML(response.text)`
+ 定位标签，提取数据 :`html.xpath('xpath的路径')`



提取直系文本：后加 `/text`,

提取该节点下 所有文本：后加 `//text`



#### 	1.4  数据清洗

+ 删除空值所在的行：`data.dropna()`
+ 为响应对象指定编码格式： `response.encoding = 'gbk'`



### 2. 爬虫进阶

#### 	2.1 Selenium模块

##### 		2.1.1 访问网页

```python
from selenium import webdriver
browser = webdriver.Chrome(executable_path='chromedriver.exe') #实例化浏览器对象
browser.get('网页')
```

|         函数或属性          |           功能           |
| :-------------------------: | :----------------------: |
|       `browser.get()`       |           访问           |
| `browser.maximize_window()` |        最大化窗口        |
|    `browser.current_url`    |    获取当前网页的url     |
|   `browser.get_cookies()`   | 获取当前网页用到的cookie |

##### 	

##### 	2.1.2  Selenium模块的定位

|     定位方式     |                             函数                             |
| :--------------: | :----------------------------------------------------------: |
|      id属性      |                  `find_element_by_id(id值)`                  |
|     name属性     |                `find_element_by_name(name值)`                |
|    class属性     |            `find_element_by_class_name(class值)`             |
|     标签名称     |             `find_element_by_tag_name(标签名称)`             |
| 链接文本精确定位 |  `find_element_by_link_text(用于精确定位链接文本的关键词)`   |
| 链接文本模糊定位 | `find_element_by_partial_link_text(用于模糊定位链接文本的关键词)` |
|    CSS选择器     |          `find_element_by_css_selector(CSS选择器)`           |
|   Xpath表达式    |             `find_element_by_xpath(XPath表达式)`             |



##### 2.1.3 Selenium模块的标签操作

| 函数或属性        | 功能               |
| ----------------- | ------------------ |
| `get_attribute()` | 获取标签的属性值   |
| `is_selected()`   | 判断标签是否被选中 |
| `is_displayed()`  | 判断标签是否显示   |
| `is_enabled()`    | 判断标签是否可用   |
| `text`            | 获取标签的文本内容 |
| `send_keys()`     | 对标签进行赋值     |
| `tag_name()`      | 获取标签的名称     |
| `size`            | 获取标签的大小     |
| `location`        | 获取标签的位置坐标 |
| `click()`         | 单击选中的标签     |



#### 2.2 Selenium模块进阶

##### 	2.2.1 模拟鼠标操作

+ 单击：

  ```python
  from selenium import webdriver
  browser = webdriver.Chrome(executable_path='chromedriver.exe') #实例化浏览器对象
  search_button = browser.find_element_bt_xpath('对应按钮路径')
  actions = ActionChains(browser)
  actions.click(search_button).perfrom()
  ```

+ 双击：`double_click()`

+ 右击：`context_click()`

+ 长按：`click_and_hold()` 

  ```python
  actions.click_and_hold(search_button).perform()
  time.sleep(5) #等待5秒
  actions.release(search_button).perform()
  ```

+ 拖动：`drag_and_drop()` & `drag_and_drop_by_offset() #表示拖动一定距离`

+ 移动：`move_by_offset() #移动一定距离` & `move_to_element() #移动到指定元素所在位置`

##### 2.2.2 显式等待和隐式等待

+ 显式等待：使用`WebDriverWait`类

```python
from selenium import webdriver
from selenium.webdriver.common.by import by
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
browser = webdriver.Chrome(executable_path='chromedriver.exe')
browser.get('url')
element = WebDriverWait(driver = browser, timeout = 10).until(EC.presence_of_element_located((by.ID,'q'))) #直到返回值不是False
element = WebDriverWait(driver = browser, timeout = 10).until_not(EC.presence_of_element_located((by.ID,'w'))) #直到返回值是False

```

+ 隐式等待（仅适用于元素缺少的情况下，超时则报错）

```python
from selenium import webdriver
from selenium.webdriver.support.ui import WebDriverWait
browser = webdriver.Chrome(executable_path='chromedriver.exe')
browser.get('url')
browser.implicitly_wait(10)  #设置时间为10秒
```



#### 	2.3 IP反爬应对

​		应对思路：使用代理服务器发起请求。创建一个代理IP池。

##### 2.3.1 搭建代理IP池

代码如下：

```python
import requests
import random

def ip_pond(url):  # 封装成函数方便使用
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36'}
    response = requests.get(uel=url, headers=headers).json()
    proxies_list = []  # ip池

    for i in response['data']:
        ip = i['ip']  # 提取每一个IP地址
        port = i['port']  # 提取每一个IP地址对应的端口
        ip_dict = {'https': f'{ip}:{port}'}  # 通过拼接字符串完成参数的格式化
        proxies_list.append(ip_dict)  # 将格式化的参数添加到代理IP池

    proxies = random.choice(proxies_list)  # 随机取出一个IP地址
    return proxies
```



#### 2.4 用Cookie池模拟登录

##### 	2.4.1 用浏览器获取cookie信息

​	选择第一个数据包，在Headers下获取Cookie信息

​	有两种方式携带cookie信息：

+ 直接在headers里带上：`headers = {'Cookie': '···', 'User-Agent':' ' }`
+ 将获取的cookie处理成字典：

```python
cookie = ' '
cookie_dict = { }
for i in cookie.split(';'):
	cookie_keys = i.split('=')[0]
	cookie_value = i.split('=')[1]
	cookue_dict[cookie_keys] = cookie_value
response = requests.get(url=url, headers=headers, cookies=cookie_dict).text
```

​	

##### 	2.4.2 自动记录cookie信息

```python
import requests

s = requests.Session()
headers = {}
data = {'email': ' ', 'password': ' '}  # 记录登陆的账号和密码信息
response = s.post(url=' ', data=data, headers=headers)  # 使用Session携带前面设置好的浏览器身份信息和账号密码信息，对网页发起请求
response1 = s.get(url='', headers=headers).text  # 再次使用Session对象对另一个需要登陆的页面发起请求，由于Session已记录上次使用的Cookie值，因此无需账号密码也可登录

```



##### 2.4.3 cookie池的搭建和使用

由于单个账号频繁登录可能被封禁，因此可以准备多个小号，搭建cookie值

```python
import requests
import random
import copy

cookie_list = []
s = requests.Session()
user_list = [
    {
        'email': '',
        'password': ''
    }
]
headers = {}
def get_cookies(user_list):  # 模拟每一个账号登录获取cookie值
    for data in user_list:
        s.post(url=' ', data=data, headers=headers)
        cookie_list.append(copy.deepcopy(s))


get_cookies(user_list)  # 调用自定义函数生成cookie池

url = ''
for i in range(1, 10):
    response = random.choice(cookie_list).get(url=url, headers=headers)

```

#### 2.5 提高爬虫效率

##### 2.5.1 多线程实现并行

暂时如此，后待更续。
